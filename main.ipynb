{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import winsound\n",
    "import mediapipe as mp\n",
    "os.environ[\"OPENCV_VIDEOIO_MSMF_ENABLE_HW_TRANSFORMS\"] = \"0\"\n",
    "import cv2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import urllib.request\n",
    "import time\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651be3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WinSoundController:\n",
    "    def __init__(self, wav_path: str):\n",
    "        self.wav_path = wav_path\n",
    "        self._on = False\n",
    "    def set_on(self, on: bool):\n",
    "        if on and not self._on:\n",
    "            winsound.PlaySound(self.wav_path,\n",
    "                               winsound.SND_FILENAME | winsound.SND_ASYNC | winsound.SND_LOOP)\n",
    "            self._on = True\n",
    "        elif not on and self._on:\n",
    "            winsound.PlaySound(None, winsound.SND_PURGE)\n",
    "            self._on = False\n",
    "\n",
    "class IRIS_TASK:\n",
    "    def __init__(self):\n",
    "        self.face_cascPath = r\"./haarcascade_frontalface_alt.xml\"\n",
    "        self.eye_cascPath  = r\"./haarcascade_eye_tree_eyeglasses.xml\"\n",
    "\n",
    "        self.faceCascade = cv2.CascadeClassifier(self.face_cascPath)\n",
    "        self.eyeCascade  = cv2.CascadeClassifier(self.eye_cascPath)\n",
    "\n",
    "        if self.faceCascade.empty() or self.eyeCascade.empty():\n",
    "            raise IOError(\"Error loading cascade XML files!\")\n",
    "\n",
    "    def process(self, frame: np.ndarray) -> Tuple[bool, str, np.ndarray]:\n",
    "\n",
    "        img = frame\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        close_eye_flag = False\n",
    "        message = \"eyes detected\"\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            face_gray  = gray[y:y+h, x:x+w]\n",
    "\n",
    "            eyes = self.eyeCascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                cv2.rectangle(img, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (255, 0, 0), 2)\n",
    "\n",
    "            if len(eyes) == 0:\n",
    "                close_eye_flag = True\n",
    "                message = \"close eye detected\"\n",
    "\n",
    "        return close_eye_flag, message, frame\n",
    "\n",
    "\n",
    "class CELL_PHONE_TASK():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        return True, \"side\", frame\n",
    "\n",
    "class HEAD_DIRECTION_TASK():\n",
    "    def __init__(self):\n",
    "        mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "        self.face_mesh = mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def get_head_direction(self, landmarks, img_w, img_h):\n",
    "        # 鼻と左右目の位置を取得\n",
    "        nose = landmarks[1]       # 鼻先\n",
    "        left_eye = landmarks[33]  # 左目端\n",
    "        right_eye = landmarks[263]# 右目端\n",
    "\n",
    "        # ピクセル座標に変換\n",
    "        nose_point = np.array([nose.x * img_w, nose.y * img_h])\n",
    "        left_point = np.array([left_eye.x * img_w, left_eye.y * img_h])\n",
    "        right_point = np.array([right_eye.x * img_w, right_eye.y * img_h])\n",
    "\n",
    "        eye_center = (left_point + right_point) / 2\n",
    "        dx = nose_point[0] - eye_center[0]\n",
    "        dy = nose_point[1] - eye_center[1]\n",
    "\n",
    "        # 閾値調整（正面を緩くするなら数値を大きく）\n",
    "        direction = \"Front\"\n",
    "        if dx > 40:\n",
    "            direction = \"Looking Right\"\n",
    "        elif dx < -40:\n",
    "            direction = \"Looking Left\"\n",
    "        elif dy > 30:\n",
    "            direction = \"Looking Down\"\n",
    "        elif dy < -30:\n",
    "            direction = \"Looking Up\"\n",
    "\n",
    "        return direction\n",
    "\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "        status = False\n",
    "        message = \"Front\"\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                direction = self.get_head_direction(face_landmarks.landmark, w, h)\n",
    "                message = direction\n",
    "                if direction != \"Front\":\n",
    "                    status = True\n",
    "                # 結果を映像に描画\n",
    "                cv2.putText(frame, f\"Direction: {direction}\", (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        return status, message, frame\n",
    "\n",
    "class HAND_TASK():\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        h, w, _,  = frame.shape\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(frame_rgb)\n",
    "\n",
    "        # Define steering wheel region (adjust this based on your camera angle)\n",
    "        roi_top_left = (int(w * 0.2), int(h * 0.5))\n",
    "        roi_bottom_right = (int(w * 0.8), int(h * 0.98))\n",
    "        cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "        hands_on_wheel = False\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                    # Check if landmark is inside steering wheel ROI\n",
    "                    if roi_top_left[0] <= x <= roi_bottom_right[0] and roi_top_left[1] <= y <= roi_bottom_right[1]:\n",
    "                        hands_on_wheel = True\n",
    "                        break\n",
    "\n",
    "        if hands_on_wheel:\n",
    "            return False, \"Hands on steering wheel\", frame\n",
    "        else:\n",
    "            return True, \"Warning: Hands removed from steering wheel!\", frame\n",
    "        \n",
    "\n",
    "def add_label(res):\n",
    "    label = res[1]\n",
    "    color = (0,0,255) if res[0] else (255,0,0)  # keep your colors\n",
    "    cv2.putText(res[2], label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    return res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "iris_task = IRIS_TASK()\n",
    "cell_phone_task = CELL_PHONE_TASK()\n",
    "head_dir_task  = HEAD_DIRECTION_TASK()\n",
    "hand_task = HAND_TASK()\n",
    "tasks = [iris_task.process, cell_phone_task.process, head_dir_task.process, hand_task.process]\n",
    "\n",
    "# Use a WAV file for reliable start/stop control\n",
    "sound = WinSoundController('alert.wav')\n",
    "verbose = True\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(tasks)) as pool:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        futures = [pool.submit(func, frame.copy()) for func in tasks]\n",
    "        results = [f.result() for f in futures]\n",
    "\n",
    "        if verbose:\n",
    "            concat_images = [add_label(res) for res in results]\n",
    "            result_image = np.concatenate(\n",
    "                (\n",
    "                    np.concatenate((concat_images[0], concat_images[1]), axis=1),\n",
    "                    np.concatenate((concat_images[2], concat_images[3]), axis=1)\n",
    "                ), axis=0\n",
    "            )\n",
    "            cv2.imshow('result', result_image)\n",
    "\n",
    "            # --- SOUND LOGIC: play once while any alert is True; stop when all False ---\n",
    "            bad_flag = any(res[0] for res in results)\n",
    "            sound.set_on(bad_flag)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "# ensure sound is stopped on exit\n",
    "sound.set_on(False)\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c615640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
