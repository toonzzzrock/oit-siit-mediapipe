{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import winsound\n",
    "import mediapipe as mp\n",
    "os.environ[\"OPENCV_VIDEOIO_MSMF_ENABLE_HW_TRANSFORMS\"] = \"0\"\n",
    "import cv2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import urllib.request\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "651be3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WinSoundController:\n",
    "    def __init__(self, wav_path: str):\n",
    "        self.wav_path = wav_path\n",
    "        self._on = False\n",
    "    def set_on(self, on: bool):\n",
    "        if on and not self._on:\n",
    "            winsound.PlaySound(self.wav_path,\n",
    "                               winsound.SND_FILENAME | winsound.SND_ASYNC | winsound.SND_LOOP)\n",
    "            self._on = True\n",
    "        elif not on and self._on:\n",
    "            winsound.PlaySound(None, winsound.SND_PURGE)\n",
    "            self._on = False\n",
    "\n",
    "class IRIS_TASK:\n",
    "    def __init__(self):\n",
    "        self.face_cascPath = r\"./haarcascade_frontalface_alt.xml\"\n",
    "        self.eye_cascPath  = r\"./haarcascade_eye_tree_eyeglasses.xml\"\n",
    "\n",
    "        self.faceCascade = cv2.CascadeClassifier(self.face_cascPath)\n",
    "        self.eyeCascade  = cv2.CascadeClassifier(self.eye_cascPath)\n",
    "\n",
    "        if self.faceCascade.empty() or self.eyeCascade.empty():\n",
    "            raise IOError(\"Error loading cascade XML files!\")\n",
    "\n",
    "    def process(self, frame: np.ndarray) -> Tuple[bool, str, np.ndarray]:\n",
    "\n",
    "        img = frame\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        close_eye_flag = False\n",
    "        message = \"eyes detected\"\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            face_gray  = gray[y:y+h, x:x+w]\n",
    "\n",
    "            eyes = self.eyeCascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                cv2.rectangle(img, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (255, 0, 0), 2)\n",
    "\n",
    "            if len(eyes) == 0:\n",
    "                close_eye_flag = True\n",
    "                message = \"close eye detected\"\n",
    "\n",
    "        return close_eye_flag, message, frame\n",
    "\n",
    "class HEAD_DIRECTION_TASK():\n",
    "    def __init__(self):\n",
    "        mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        # 時間管理用変数\n",
    "        self.off_start_time = None\n",
    "        self.warning_active = False\n",
    "\n",
    "    def get_head_direction(self, landmarks, img_w, img_h):\n",
    "        # 鼻と左右目の位置を取得\n",
    "        nose = landmarks[1]        # 鼻先\n",
    "        left_eye = landmarks[33]   # 左目端\n",
    "        right_eye = landmarks[263] # 右目端\n",
    "\n",
    "        # ピクセル座標に変換\n",
    "        nose_point = np.array([nose.x * img_w, nose.y * img_h])\n",
    "        left_point = np.array([left_eye.x * img_w, left_eye.y * img_h])\n",
    "        right_point = np.array([right_eye.x * img_w, right_eye.y * img_h])\n",
    "\n",
    "        eye_center = (left_point + right_point) / 2\n",
    "        dx = nose_point[0] - eye_center[0]\n",
    "        dy = nose_point[1] - eye_center[1]\n",
    "\n",
    "        # 閾値調整\n",
    "        direction = \"Front\"\n",
    "        if dx > 40:\n",
    "            direction = \"Looking Right\"\n",
    "        elif dx < -40:\n",
    "            direction = \"Looking Left\"\n",
    "        elif dy > 30:\n",
    "            direction = \"Looking Down\"\n",
    "        elif dy < -30:\n",
    "            direction = \"Looking Up\"\n",
    "\n",
    "        return direction\n",
    "\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "        status = False\n",
    "        message = \"Front\"\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                direction = self.get_head_direction(face_landmarks.landmark, w, h)\n",
    "                message = direction\n",
    "\n",
    "                # 顔の枠描画\n",
    "                x_coords = [lm.x * w for lm in face_landmarks.landmark]\n",
    "                y_coords = [lm.y * h for lm in face_landmarks.landmark]\n",
    "                x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "                y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Front 以外なら時間を測定\n",
    "                if direction != \"Front\":\n",
    "                    if self.off_start_time is None:\n",
    "                        self.off_start_time = time.time()\n",
    "                    else:\n",
    "                        elapsed = time.time() - self.off_start_time\n",
    "                        if elapsed >= 3:  # 3秒以上\n",
    "                            self.warning_active = True\n",
    "                else:\n",
    "                    # 正面を向いたらリセット\n",
    "                    self.off_start_time = None\n",
    "                    self.warning_active = False\n",
    "\n",
    "                # 警告表示\n",
    "                if self.warning_active:\n",
    "                    status = True\n",
    "                    cv2.putText(frame, \"WARNING: Distracted!\", (30, 100),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "                # 現在の向きを描画\n",
    "                cv2.putText(frame, f\"Direction: {direction}\", (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        return status, message, frame\n",
    "\n",
    "class HAND_TASK():\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        h, w, _,  = frame.shape\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(frame_rgb)\n",
    "\n",
    "        # Define steering wheel region (adjust this based on your camera angle)\n",
    "        roi_top_left = (int(w * 0.2), int(h * 0.5))\n",
    "        roi_bottom_right = (int(w * 0.8), int(h * 0.98))\n",
    "        cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "        hands_on_wheel = False\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                    # Check if landmark is inside steering wheel ROI\n",
    "                    if roi_top_left[0] <= x <= roi_bottom_right[0] and roi_top_left[1] <= y <= roi_bottom_right[1]:\n",
    "                        hands_on_wheel = True\n",
    "                        break\n",
    "\n",
    "        if hands_on_wheel:\n",
    "            return False, \"Hands on steering wheel\", frame\n",
    "        else:\n",
    "            return True, \"Warning: Hands removed from steering wheel!\", frame\n",
    "        \n",
    "\n",
    "def add_label(res):\n",
    "    label = res[1]\n",
    "    color = (0,0,255) if res[0] else (255,0,0)  # keep your colors\n",
    "    cv2.putText(res[2], label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    return res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developers.google.com/mediapipe/solutions/vision/object_detector\n",
    "class CELL_PHONE_TASK():\n",
    "    # https://storage.googleapis.com/mediapipe-models/\n",
    "    base_url = 'https://storage.googleapis.com/mediapipe-tasks/object_detector/'\n",
    "    model_name = 'efficientdet_lite0_fp32.tflite'\n",
    "    model_folder_path = './learned_models/mediapipe'\n",
    "\n",
    "    H_MARGIN = 10  # pixels\n",
    "    V_MARGIN = 30  # pixels\n",
    "    FONT_SIZE = 1\n",
    "    FONT_THICKNESS = 1\n",
    "    TEXT_COLOR = (0, 255, 0)  # green\n",
    "\n",
    "    # full list(efficientdet_lite0_fp32.tflite)\n",
    "    # https://storage.googleapis.com/mediapipe-tasks/object_detector/labelmap.txt\n",
    "\n",
    "    # https://developers.google.com/mediapipe/solutions/vision/object_detector\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_folder_path=model_folder_path,\n",
    "            base_url=base_url,\n",
    "            model_name=model_name,\n",
    "            max_results=2,\n",
    "            score_threshold=0.05,\n",
    "            mode=\"video\"\n",
    "            ):\n",
    "\n",
    "        self.mode = mode\n",
    "        if self.mode == \"image\":\n",
    "            rmode = mp.tasks.vision.RunningMode.IMAGE\n",
    "        else:\n",
    "            rmode = mp.tasks.vision.RunningMode.VIDEO\n",
    "\n",
    "\n",
    "        model_path = self.set_model(base_url, model_folder_path, model_name)\n",
    "        options = mp.tasks.vision.ObjectDetectorOptions(\n",
    "            base_options=mp.tasks.BaseOptions(model_asset_path=model_path),\n",
    "            max_results=max_results,\n",
    "            score_threshold=score_threshold,\n",
    "            running_mode=rmode\n",
    "        )\n",
    "        self.detector = mp.tasks.vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    def set_model(self, base_url, model_folder_path, model_name):\n",
    "        model_path = model_folder_path+'/'+model_name\n",
    "        # download model if model file does not exist\n",
    "        if not os.path.exists(model_path):\n",
    "            # make directory if model_folder directory does not exist\n",
    "            os.makedirs(model_folder_path, exist_ok=True)\n",
    "            # download model file\n",
    "            url = base_url+model_name\n",
    "            save_name = model_path\n",
    "            urllib.request.urlretrieve(url, save_name)\n",
    "        return model_path\n",
    "\n",
    "    def process(self, frame) -> Tuple[bool, str, np.ndarray]:\n",
    "        detected = False\n",
    "        message = \"No object detected in hand\"\n",
    "\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        hand_results = self.hands.process(img_rgb)\n",
    "\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            print(f\"Detected {len(hand_results.multi_hand_landmarks)} hands\")\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                # Get hand bounding box\n",
    "                x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)\n",
    "                y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)\n",
    "                x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)\n",
    "                y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)\n",
    "\n",
    "                pad = 10\n",
    "                x_min = max(x_min - pad, 0)\n",
    "                y_min = max(y_min - pad, 0)\n",
    "                x_max = min(x_max + pad, w)\n",
    "                y_max = min(y_max + pad, h)\n",
    "\n",
    "                # Crop the hand region\n",
    "                hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "                if hand_crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Convert to MediaPipe Image\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.ascontiguousarray(cv2.cvtColor(hand_crop, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "                # Detect objects in hand\n",
    "                detection_result = self.detector.detect_for_video(mp_image, int(time.time() * 1000))\n",
    "                print(detection_result)\n",
    "\n",
    "                for det in detection_result.detections:\n",
    "                    category = det.categories[0].category_name or \"object\"\n",
    "                    score = det.categories[0].score\n",
    "                    bbox = det.bounding_box\n",
    "\n",
    "                    # Filter out hand itself\n",
    "                    if category.lower() not in [\"cell phone\", \"cellphone\", \"mobile phone\", \"phone\"]:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    detected = True\n",
    "                    message = f\"{category} ({score:.2f})\"\n",
    "\n",
    "                    # Map bbox to original frame coordinates\n",
    "                    abs_x_min = x_min + int(bbox.origin_x)\n",
    "                    abs_y_min = y_min + int(bbox.origin_y)\n",
    "                    abs_x_max = x_min + int(bbox.origin_x + bbox.width)\n",
    "                    abs_y_max = y_min + int(bbox.origin_y + bbox.height)\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (abs_x_min, abs_y_min), (abs_x_max, abs_y_max), (0, 0, 255), 2)\n",
    "                    cv2.putText(frame, message, (abs_x_min, abs_y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "                # Draw hand landmarks for reference\n",
    "                self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        print(detected, message)\n",
    "        return detected, message, frame\n",
    "        # return True, \"cell phone in hand\", frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77f4b0f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file at c:\\oit\\py25en\\WPy64-312101\\python\\Lib\\site-packages/C:\\oit\\py25en\\source\\projects\\learned_models\\mediapipe\\efficientdet_lite0_fp32.tflite, errno=22",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cap\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_BUFFERSIZE, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m iris_task \u001b[38;5;241m=\u001b[39m IRIS_TASK()\n\u001b[1;32m----> 5\u001b[0m cell_phone_task \u001b[38;5;241m=\u001b[39m \u001b[43mCELL_PHONE_TASK\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m head_dir_task  \u001b[38;5;241m=\u001b[39m HEAD_DIRECTION_TASK()\n\u001b[0;32m      7\u001b[0m hand_task \u001b[38;5;241m=\u001b[39m HAND_TASK()\n",
      "Cell \u001b[1;32mIn[19], line 43\u001b[0m, in \u001b[0;36mCELL_PHONE_TASK.__init__\u001b[1;34m(self, model_folder_path, base_url, model_name, max_results, score_threshold, mode)\u001b[0m\n\u001b[0;32m     36\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moit\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpy25en\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprojects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlearned_models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmediapipe\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mefficientdet_lite0_fp32.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvision\u001b[38;5;241m.\u001b[39mObjectDetectorOptions(\n\u001b[0;32m     38\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39mmodel_path),\n\u001b[0;32m     39\u001b[0m     max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[0;32m     40\u001b[0m     score_threshold\u001b[38;5;241m=\u001b[39mscore_threshold,\n\u001b[0;32m     41\u001b[0m     running_mode\u001b[38;5;241m=\u001b[39mrmode\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObjectDetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhands \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_hands\u001b[38;5;241m.\u001b[39mHands(\n\u001b[0;32m     47\u001b[0m     static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m     max_num_hands\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     49\u001b[0m     min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     50\u001b[0m     min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mc:\\oit\\py25en\\WPy64-312101\\python\\Lib\\site-packages\\mediapipe\\tasks\\python\\vision\\object_detector.py:238\u001b[0m, in \u001b[0;36mObjectDetector.create_from_options\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m    220\u001b[0m   options\u001b[38;5;241m.\u001b[39mresult_callback(\n\u001b[0;32m    221\u001b[0m       detection_result,\n\u001b[0;32m    222\u001b[0m       image,\n\u001b[0;32m    223\u001b[0m       timestamp\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND,\n\u001b[0;32m    224\u001b[0m   )\n\u001b[0;32m    226\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[0;32m    227\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[0;32m    228\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    237\u001b[0m )\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\oit\\py25en\\WPy64-312101\\python\\Lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[1;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     67\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     69\u001b[0m   )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner \u001b[38;5;241m=\u001b[39m \u001b[43m_TaskRunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m=\u001b[39m running_mode\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open file at c:\\oit\\py25en\\WPy64-312101\\python\\Lib\\site-packages/C:\\oit\\py25en\\source\\projects\\learned_models\\mediapipe\\efficientdet_lite0_fp32.tflite, errno=22"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "iris_task = IRIS_TASK()\n",
    "cell_phone_task = CELL_PHONE_TASK()\n",
    "head_dir_task  = HEAD_DIRECTION_TASK()\n",
    "hand_task = HAND_TASK()\n",
    "tasks = [iris_task.process, cell_phone_task.process, head_dir_task.process, hand_task.process]\n",
    "\n",
    "# Use a WAV file for reliable start/stop control\n",
    "sound = WinSoundController('alert.wav')\n",
    "verbose = True\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(tasks)) as pool:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        futures = [pool.submit(func, frame.copy()) for func in tasks]\n",
    "        results = [f.result() for f in futures]\n",
    "\n",
    "        if verbose:\n",
    "            concat_images = [add_label(res) for res in results]\n",
    "            result_image = np.concatenate(\n",
    "                (\n",
    "                    np.concatenate((concat_images[0], concat_images[1]), axis=1),\n",
    "                    np.concatenate((concat_images[2], concat_images[3]), axis=1)\n",
    "                ), axis=0\n",
    "            )\n",
    "            cv2.imshow('result', result_image)\n",
    "\n",
    "            # --- SOUND LOGIC: play once while any alert is True; stop when all False ---\n",
    "            bad_flag = any(res[0] for res in results)\n",
    "            sound.set_on(bad_flag)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "# ensure sound is stopped on exit\n",
    "sound.set_on(False)\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826ce2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
